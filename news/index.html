<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> news | Wen-Chin Huang 黃文勁 </title> <meta name="author" content="Wen-Chin Huang"> <meta name="description" content="Assistant Professor, Nagoya University. "> <meta name="keywords" content="voice conversion,"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://unilight.github.io/news/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Wen-Chin Huang 黃文勁 </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item "> <a class="nav-link" href="/about_jp/">略歴 </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">Blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">Repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/materials/">Materials </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">news</h1> <p class="post-description"></p> </header> <article> <div class="news"> <div class="table-responsive"> <table class="table table-sm table-borderless"> <tr> <th scope="row" style="width: 20%">Aug 21, 2025</th> <td> Two papers <a href="first%20author">[SHEET](https://arxiv.org/abs/2505.15061)</a> <a href="co-author">[Language-independent speaker anonymization](https://arxiv.org/abs/2507.00458)</a> were presented at <strong>Interspeech 2025</strong>. </td> </tr> <tr> <th scope="row" style="width: 20%">Aug 17, 2025</th> <td> I gave a tutorial at <strong>Interspeech 2025</strong> wiith Erica Cooper (NICT, Japan) and Jiatong Shi (CMU, USA) on the topic “Automatic Quality Assessment for Speech and Beyond”. [<a href="https://voicemos-challenge-2023.github.io/speech-synthesis-evaluation/IS2025_tutorial.pdf" rel="external nofollow noopener" target="_blank">Slides</a>] </td> </tr> <tr> <th scope="row" style="width: 20%">Jun 25, 2025</th> <td> The <a href="https://sites.google.com/view/voicemos-challenge/audiomos-challenge-2025" rel="external nofollow noopener" target="_blank">AudioMOS Challenge 2025</a> is officially over! Now you can freely get the datasets on the challenge page. There will also be a <a href="https://2025.ieeeasru.org/program/challenges" rel="external nofollow noopener" target="_blank">special session at ASRU 2025</a>! </td> </tr> <tr> <th scope="row" style="width: 20%">May 15, 2025</th> <td> I gave an online invited talk at the <a href="https://poonehmousavi.github.io/rg" rel="external nofollow noopener" target="_blank">Conversational AI Reading Group, MILA</a>. The topic was “Automatic Quality Assessment for Speech and Beyond”. [<a href="../assets/others/20250515-convai-mila-rg.pdf">Slides</a>] </td> </tr> <tr> <th scope="row" style="width: 20%">Apr 09, 2025</th> <td> The <a href="https://vc-challenge.org/" rel="external nofollow noopener" target="_blank">Singing Voice Conversion Challenge 2025</a> kicks off today! This year we focus on singing <strong>technique</strong> conversion. If you are interested in participating, please register here and we will contact you! https://forms.gle/GZGAWJAZvgDK6QKcA </td> </tr> <tr> <th scope="row" style="width: 20%">Apr 09, 2025</th> <td> The <a href="https://sites.google.com/view/voicemos-challenge/audiomos-challenge-2025?authuser=0" rel="external nofollow noopener" target="_blank">AudioMOS Challenge 2025</a> kicks off today! Participants will build quality predictors for speehc, music and general audio. We are still accepting new challengers! If you are interested in participating, please register here and we will contact you! https://forms.gle/am1qDtEwWVmEnh5d9 </td> </tr> <tr> <th scope="row" style="width: 20%">Apr 08, 2025</th> <td> One paper was presented at <strong>ICASSP 2025</strong>. [<a href="https://ieeexplore.ieee.org/abstract/document/10889744" rel="external nofollow noopener" target="_blank">Investigating Factors Related to the Naturalness of Synthesized Unison Singing</a>] </td> </tr> <tr> <th scope="row" style="width: 20%">Dec 02, 2024</th> <td> One paper was presented at <strong>SLT2024</strong>. [<a href="https://arxiv.org/abs/2409.07001" rel="external nofollow noopener" target="_blank">VoiceMOS Challenge 2024</a>] </td> </tr> <tr> <th scope="row" style="width: 20%">Nov 07, 2024</th> <td> A new preprint is now available [<a href="https://arxiv.org/abs/2411.03715" rel="external nofollow noopener" target="_blank">MOS-Bench</a>]. The corresponding open-source toolkit, [<a href="https://github.com/unilight/sheet" rel="external nofollow noopener" target="_blank">SHEET</a>], is also available on GitHub. </td> </tr> <tr> <th scope="row" style="width: 20%">Oct 25, 2024</th> <td> I gave an <a href="https://ken.ieice.org/ken/program/index.php?tgs_regid=1e123e2e4f2c2b52bf6e5759b39cf1404e85a5e112804baf829d6e661762190f&amp;tgid=IEICE-SP" rel="external nofollow noopener" target="_blank">invited talk</a> on voice conversion at SP/IPSJ-SLP. Please find the <a href="../assets/others/20241025-spslp.pptx">slides</a> here. </td> </tr> <tr> <th scope="row" style="width: 20%">Oct 17, 2024</th> <td> A paper was published at IEEE Signal Processing Letters (acceptance rate: 10-20%). [<a href="https://ieeexplore.ieee.org/document/10720809" rel="external nofollow noopener" target="_blank">SA-TTS</a>] </td> </tr> <tr> <th scope="row" style="width: 20%">Aug 14, 2024</th> <td> I gave an <a href="https://www.citi.sinica.edu.tw/assets/htmls/IZ240024_zh.html" rel="external nofollow noopener" target="_blank">invited talk</a> on voice conversion at CITI, Academia Sinica, Taiwan. Please find the <a href="../assets/others/20240814-citi-talk.pptx">slides</a> here. </td> </tr> <tr> <th scope="row" style="width: 20%">Jul 16, 2024</th> <td> I gave a lecture on voice conversion. Please find the <a href="../assets/pdf/20240716-class.pdf">slides</a> here. </td> </tr> <tr> <th scope="row" style="width: 20%">Jul 01, 2024</th> <td> We wrote a review paper on evaluation of synthesis speech, which was published at Acoustical Science and Technology, a journal in Japan. The English version can be found <a href="https://www.jstage.jst.go.jp/article/ast/45/4/45_e24.12/_article/-char/ja" rel="external nofollow noopener" target="_blank">here</a>. </td> </tr> <tr> <th scope="row" style="width: 20%">Jun 14, 2024</th> <td> The <a href="https://sites.google.com/view/voicemos-challenge/past-challenges/voicemos-challenge-2024" rel="external nofollow noopener" target="_blank">VoiceMOS Challenge 2024</a> is officially over! Now you can freely get the datasets by registering through the <a href="https://www.codabench.org/competitions/2650/" rel="external nofollow noopener" target="_blank">CodaBench page</a>. There will also be a <a href="https://2024.ieeeslt.org/challenges/" rel="external nofollow noopener" target="_blank">special session at SLT 2024</a>! </td> </tr> <tr> <th scope="row" style="width: 20%">May 17, 2024</th> <td> One paper was accepted to <strong>IEEE/ACM TASLP</strong>. [<a href="https://ieeexplore.ieee.org/abstract/document/10533680" rel="external nofollow noopener" target="_blank">Pretraining and Adaptation Techniques for Electrolaryngeal Speech Recognition</a>] </td> </tr> <tr> <th scope="row" style="width: 20%">Apr 16, 2024</th> <td> One paper was accepted to <strong>IEEE/ACM TASLP</strong>. [<a href="https://arxiv.org/abs/2404.09385" rel="external nofollow noopener" target="_blank">A Large-Scale Evaluation of Speech Foundation Models</a>] </td> </tr> <tr> <th scope="row" style="width: 20%">Apr 14, 2024</th> <td> One paper was presented at <strong>ICASSP 2024</strong>. [<a href="https://arxiv.org/abs/2309.09627" rel="external nofollow noopener" target="_blank">Electrolaryngeal Speech Intelligibility Enhancement through Robust Linguistic Encoders</a>] </td> </tr> <tr> <th scope="row" style="width: 20%">Apr 01, 2024</th> <td> I am now an assistant professor at the Graduate School of Informatics, Nagoya University. </td> </tr> <tr> <th scope="row" style="width: 20%">Feb 16, 2024</th> <td> I successfully defended my <a href="../assets/pdf/phd-thesis.pdf">Ph.D. thesis</a>! </td> </tr> <tr> <th scope="row" style="width: 20%">Dec 15, 2023</th> <td> Four papers were presented at <strong>ASRU 2023</strong>. [<a href="https://arxiv.org/abs/2306.14422" rel="external nofollow noopener" target="_blank">SVCC2023</a>] [<a href="https://arxiv.org/abs/2310.02640" rel="external nofollow noopener" target="_blank">VoiceMOS Challenge 2023</a>] [<a href="https://arxiv.org/abs/2310.05203" rel="external nofollow noopener" target="_blank">NU-SVCC2023</a>] [<a href="https://arxiv.org/abs/2310.02570" rel="external nofollow noopener" target="_blank">N2D-VC-GST</a>] </td> </tr> <tr> <th scope="row" style="width: 20%">Aug 25, 2023</th> <td> A paper was accepted to <strong>APSIPA ASC 2023</strong>. [<a href="https://arxiv.org/abs/2309.02133" rel="external nofollow noopener" target="_blank">Evaluate-FAC</a>] </td> </tr> <tr> <th scope="row" style="width: 20%">Jun 26, 2023</th> <td> The <a href="http://vc-challenge.org/" rel="external nofollow noopener" target="_blank">Singing Voice Conversion Challenge 2023</a> is over! We have a <a href="https://arxiv.org/abs/2306.14422" rel="external nofollow noopener" target="_blank">summary paper</a> submitted to arXiv. There will also be a <a href="http://www.asru2023.org/motion.asp?siteid=1007526&amp;menuid=49656&amp;postid=697225&amp;lgid=1" rel="external nofollow noopener" target="_blank">special session at ASRU 2023</a>! </td> </tr> <tr> <th scope="row" style="width: 20%">Jun 05, 2023</th> <td> I was honored the Outstanding Graduate Student Award (学術奨励賞) of Nagoya University! </td> </tr> <tr> <th scope="row" style="width: 20%">Apr 10, 2023</th> <td> I start serving as a student researcher at <strong>Google Japan</strong>. </td> </tr> <tr> <th scope="row" style="width: 20%">Apr 07, 2023</th> <td> I open-sourced the <a href="https://github.com/unilight/seq2seq-vc" rel="external nofollow noopener" target="_blank"><strong>seq2seq-vc</strong></a> toolkit! It is a toolkit for sequence-to-sequence voice conversion research. Please check it out! </td> </tr> <tr> <th scope="row" style="width: 20%">Mar 15, 2023</th> <td> I open-sourced the <a href="https://github.com/unilight/s3prl-vc" rel="external nofollow noopener" target="_blank"><strong>s3prl-vc</strong></a> toolkit! It also comes with a <a href="https://huggingface.co/spaces/unilight/s3prl-vc-vcc2020" rel="external nofollow noopener" target="_blank">HuggingFace Spaces demo</a>. Please check them out! </td> </tr> <tr> <th scope="row" style="width: 20%">Jan 19, 2023</th> <td> The first <a href="http://www.vc-challenge.org/" rel="external nofollow noopener" target="_blank">Singing Voice Conversion Challenge</a> kicks off today! This is a new version of the voice conversion challenge (VCC) series that aims to compare techniques for singing voice conversion, in contrast to normal voice conversion. We are still accepting new challengers! If you are interested in participating, please fill in the <a href="https://forms.gle/2Xc9Vb39vjhX72hA6" rel="external nofollow noopener" target="_blank">registration form</a>. </td> </tr> <tr> <th scope="row" style="width: 20%">Dec 27, 2022</th> <td> The VTN journal paper received the <strong>16th IEEE Signal Processing Society Japan Student Best Paper Award</strong>. <a href="https://ieeexplore.ieee.org/document/9314100" rel="external nofollow noopener" target="_blank">Open access</a>. </td> </tr> <tr> <th scope="row" style="width: 20%">Jul 25, 2022</th> <td> One journal was accepted to the <strong>IEEE Journal of Selected Topics in Signal Processing</strong>. <a href="https://arxiv.org/abs/2207.04356" rel="external nofollow noopener" target="_blank">ArXiv version</a>. </td> </tr> <tr> <th scope="row" style="width: 20%">Jun 14, 2022</th> <td> One paper [<a href="https://arxiv.org/abs/2301.10606" rel="external nofollow noopener" target="_blank">Expressive Speech-to-Speech Translation</a>] was accepted to <strong>ICASSP 2023</strong>. Also, one paper I co-authored [<a href="https://arxiv.org/abs/2211.01079" rel="external nofollow noopener" target="_blank">Intermediate fine-tuning for pathological ASR</a>] was also accepted. </td> </tr> <tr> <th scope="row" style="width: 20%">Jun 14, 2022</th> <td> Two papers [<a href="https://arxiv.org/abs/2207.03697" rel="external nofollow noopener" target="_blank">End-to-end binaural synthesis</a>] [<a href="https://arxiv.org/abs/2203.11389" rel="external nofollow noopener" target="_blank">VoiceMOS Challenge 2022</a>] were accepted to <strong>Interspeech 2022</strong>. Also, one paper I co-authored [<a href="https://arxiv.org/abs/2203.15431" rel="external nofollow noopener" target="_blank">SSL for pathological ASR</a>] was also accepted. </td> </tr> <tr> <th scope="row" style="width: 20%">May 16, 2022</th> <td> I started my internship at <strong>FAIR (Fundamental AI Research), Meta</strong>. </td> </tr> <tr> <th scope="row" style="width: 20%">Mar 23, 2022</th> <td> I was invited to give a talk at 音声言語情報処理研究会/音声研究会 (SLP/SP), a Japanese domestic conference. Slides are [<a href="https://www.slideshare.net/NU_I_TODALAB/the-voicemos-challenge-2022" rel="external nofollow noopener" target="_blank">here</a>]. </td> </tr> <tr> <th scope="row" style="width: 20%">Mar 23, 2022</th> <td> The <a href="https://voicemos-challenge-2022.github.io/" rel="external nofollow noopener" target="_blank">VoiceMOS Challenge 2022</a> is over! We have a [<a href="https://arxiv.org/abs/2203.11389" rel="external nofollow noopener" target="_blank">summary paper</a>] submitted to arXiv. The <a href="https://codalab.lisn.upsaclay.fr/competitions/695" rel="external nofollow noopener" target="_blank">CodaLab competition page</a> is still opened, and ANYONE can register to get the dataset and give it a try! </td> </tr> <tr> <th scope="row" style="width: 20%">Jan 22, 2022</th> <td> Two first-author papers [<a href="https://arxiv.org/abs/2110.06280" rel="external nofollow noopener" target="_blank">S3PRL-VC</a>] [<a href="https://arxiv.org/abs/2110.09103" rel="external nofollow noopener" target="_blank">LDNet</a>] and one co-first author paper [<a href="https://arxiv.org/abs/2110.08213" rel="external nofollow noopener" target="_blank">N2D VC</a>] were accepted to <strong>ICASSP 2022</strong>. Also, two papers I co-authored [<a href="https://arxiv.org/abs/2110.02635" rel="external nofollow noopener" target="_blank">mos-finetune-ssl</a>] [<a href="https://arxiv.org/abs/2111.07116" rel="external nofollow noopener" target="_blank">Direct N2N VC</a>] were also accepted. </td> </tr> <tr> <th scope="row" style="width: 20%">Jan 13, 2022</th> <td> The <a href="https://voicemos-challenge-2022.github.io/" rel="external nofollow noopener" target="_blank">VoiceMOS Challenge</a> was accepted as a <a href="https://interspeech2022.org/program/special.php" rel="external nofollow noopener" target="_blank">special session at INTERSPEECH 2022</a>! Again, we are still accepting new challengers. If you are interested in participating, please contact us at <a href="voicemos2022@nii.ac.jp">voicemos2022@nii.ac.jp</a> first then register at the <a href="https://codalab.lisn.upsaclay.fr/competitions/695" rel="external nofollow noopener" target="_blank">CodaLab page</a>. </td> </tr> <tr> <th scope="row" style="width: 20%">Dec 20, 2021</th> <td> The first <a href="https://nii-yamagishilab.github.io/ecooper-demo/VoiceMOS2022/index.html" rel="external nofollow noopener" target="_blank">VoiceMOS Challenge</a> kicks off today! This is a new challenge that aims to compare techniques for predicting the mean opinion score (MOS) of synthetic speech. We are still accepting new challengers! If you are interested in participating, please contact us at <a href="voicemos2022@nii.ac.jp">voicemos2022@nii.ac.jp</a>. </td> </tr> <tr> <th scope="row" style="width: 20%">Dec 17, 2021</th> <td> Received the Best Paper Award at <strong>APSIPA ASC 2021</strong>! </td> </tr> <tr> <th scope="row" style="width: 20%">Sep 11, 2021</th> <td> One first-author paper [<a href="https://arxiv.org/abs/2107.09477" rel="external nofollow noopener" target="_blank">Prosody for ASR+TTS VC</a>] was accepted to <strong>ASRU 2021</strong>. Also, one paper I co-authored [ELVC w/ Seq2seq] was accepted. </td> </tr> <tr> <th scope="row" style="width: 20%">Aug 31, 2021</th> <td> Three co-author papers were accepted to <strong>APSIPA ASC 2021</strong>. [<a href="https://arxiv.org/abs/2109.03551" rel="external nofollow noopener" target="_blank">ELVC w/ lip</a>] [Noisy-to-noisy VC] [Investigation of non-parallel seq2seq VC w/ synthetic data] </td> </tr> <tr> <th scope="row" style="width: 20%">Aug 31, 2021</th> <td> I started my internship at <strong>Facebook Reality Labs Research</strong>. </td> </tr> <tr> <th scope="row" style="width: 20%">Jul 27, 2021</th> <td> You can read some posts I wrote in the blog page, as long as you understand Mandarin Chinese. </td> </tr> <tr> <th scope="row" style="width: 20%">Jun 04, 2021</th> <td> One first-author paper [<a href="https://arxiv.org/abs/2106.01415" rel="external nofollow noopener" target="_blank">Dysarthric VC w/ VTN+VAE</a>] was accepted to <strong>Interspeech 2021</strong>. Also, one paper I co-authored <a href="https://arxiv.org/abs/2106.05629" rel="external nofollow noopener" target="_blank">[Relational data selection]</a> was accepted. </td> </tr> <tr> <th scope="row" style="width: 20%">Feb 22, 2021</th> <td> I successfully defensed my <a href="../assets/pdf/master-thesis.pdf" target="_blank">master’s thesis</a>. Also, I successfully passed the Ph.D. entrance exam, and will become a <strong>Ph.D. candidate</strong> at the Graduate School of Informatics, Nagoya University. </td> </tr> <tr> <th scope="row" style="width: 20%">Jan 30, 2021</th> <td> One paper [<a href="https://arxiv.org/abs/2102.03786" rel="external nofollow noopener" target="_blank">EMA2S</a>] was accepted to <strong>IEEE International Symposium on Circuits and Systems (ISCAS) 2021</strong>. </td> </tr> <tr> <th scope="row" style="width: 20%">Jan 30, 2021</th> <td> Two first-author papers [<a href="https://arxiv.org/abs/2010.12231" rel="external nofollow noopener" target="_blank">VQVAE-VC</a>] [<a href="https://arxiv.org/abs/2102.00291" rel="external nofollow noopener" target="_blank">BERT-ASR</a>] were accepted to <strong>ICASSP 2021</strong>. Also, two papers I co-authored [<a href="https://github.com/k2kobayashi/crank" rel="external nofollow noopener" target="_blank">crank</a>] [<a href="https://kan-bayashi.github.io/NonARSeq2SeqVC/" rel="external nofollow noopener" target="_blank">NonAR seq2seq VC</a>] were also accepted. </td> </tr> <tr> <th scope="row" style="width: 20%">Jan 07, 2021</th> <td> One journal was accepted to the <strong>IEEE/ACM Transactions on Audio, Speech, and Language Processing</strong>. The <a href="https://ieeexplore.ieee.org/document/9314100" rel="external nofollow noopener" target="_blank">early access version</a> is available now on IEEE Xplore. There is also an <a href="https://arxiv.org/abs/2008.03088" rel="external nofollow noopener" target="_blank">arXiv version</a>. </td> </tr> <tr> <th scope="row" style="width: 20%">Oct 17, 2020</th> <td> Four papers are accepted to the <a href="https://www.synsig.org/index.php/Joint_Workshop_for_the_Blizzard_Challenge_and_Voice_Conversion_Challenge_2020" rel="external nofollow noopener" target="_blank">Joint Workshop for the Blizzard Challenge and Voice Conversion Challenge 2020</a>! [<a href="https://www.isca-speech.org/archive/VCC_BC_2020/pdfs/VCC2020_paper_13.pdf" rel="external nofollow noopener" target="_blank">Challenge Summary</a>] [<a href="https://www.isca-speech.org/archive/VCC_BC_2020/pdfs/VCC2020_paper_34.pdf" rel="external nofollow noopener" target="_blank">Objective Assesement</a>] [<a href="https://www.isca-speech.org/archive/VCC_BC_2020/pdfs/VCC2020_paper_11.pdf" rel="external nofollow noopener" target="_blank">Baseline ASR+TTS</a>] [<a href="https://www.isca-speech.org/archive/VCC_BC_2020/pdfs/VCC2020_paper_36.pdf" rel="external nofollow noopener" target="_blank">NU entry</a>] </td> </tr> <tr> <th scope="row" style="width: 20%">Oct 17, 2020</th> <td> The <a href="https://www.isca-speech.org/archive/VCC_BC_2020/" rel="external nofollow noopener" target="_blank">proceeding</a> of the <a href="https://www.synsig.org/index.php/Joint_Workshop_for_the_Blizzard_Challenge_and_Voice_Conversion_Challenge_2020" rel="external nofollow noopener" target="_blank">Joint Workshop for the Blizzard Challenge and Voice Conversion Challenge 2020</a> is online now! </td> </tr> <tr> <th scope="row" style="width: 20%">Jul 29, 2020</th> <td> The implementation of <a href="https://github.com/espnet/espnet/tree/master/egs/arctic/vc1" rel="external nofollow noopener" target="_blank">VTN</a> is open-sourced on <strong>ESPnet</strong>. </td> </tr> <tr> <th scope="row" style="width: 20%">Jul 28, 2020</th> <td> One paper [<a href="https://unilight.github.io/Publication-Demos/publications/transformer-vc">VTN</a>] was accepted to <strong>Interspeech 2020</strong>. </td> </tr> <tr> <th scope="row" style="width: 20%">May 20, 2020</th> <td> One journal paper [<a href="https://www.sciencedirect.com/science/article/abs/pii/S0885230820300474" rel="external nofollow noopener" target="_blank">ASVspoof 2019 database</a>] was accepted to the <strong>Computer Speech &amp; Language</strong>. </td> </tr> <tr> <th scope="row" style="width: 20%">Mar 09, 2020</th> <td> I am co-organizing the <a href="http://www.vc-challenge.org/" rel="external nofollow noopener" target="_blank">Voice Conversion Challenge 2020</a>. I developed a <a href="https://github.com/espnet/espnet/tree/master/egs/vcc20" rel="external nofollow noopener" target="_blank">seq-to-seq baseline w/ ESPnet</a>. </td> </tr> <tr> <th scope="row" style="width: 20%">Jan 19, 2020</th> <td> One journal paper [<a href="https://arxiv.org/pdf/2001.07849.pdf" rel="external nofollow noopener" target="_blank">CDVAE-CLS-GAN</a>] was accepted to the <strong>IEEE Transactions on Emerging Topics in Computational Intelligence</strong>. </td> </tr> </table> </div> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Wen-Chin Huang. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. Last updated: August 26, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?601a2d3465e2a52bec38b600518d5f70"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"About",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-\u7565\u6b74",title:"\u7565\u6b74",description:"",section:"Navigation",handler:()=>{window.location.href="/about_jp/"}},{id:"nav-blog",title:"Blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-publications",title:"Publications",description:"publications by categories in reversed chronological order. generated by jekyll-scholar.",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"nav-repositories",title:"Repositories",description:"",section:"Navigation",handler:()=>{window.location.href="/repositories/"}},{id:"nav-materials",title:"Materials",description:"Materials for courses I taught or talks I gave.",section:"Navigation",handler:()=>{window.location.href="/materials/"}},{id:"post-\u540d\u53e4\u5c4b\u5927\u5b78\u52a9\u7406\u6559\u63881\u5e74\u76ee",title:'\u540d\u53e4\u5c4b\u5927\u5b78\u52a9\u7406\u6559\u63881\u5e74\u76ee <svg width="1.2rem" height="1.2rem" top=".5rem" viewBox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"><path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg>',description:"",section:"Posts",handler:()=>{window.open("https://medium.com/@unilight/%E5%90%8D%E5%8F%A4%E5%B1%8B%E5%A4%A7%E5%AD%B8%E5%8A%A9%E7%90%86%E6%95%99%E6%8E%881%E5%B9%B4%E7%9B%AE-1f418968c885?source=rss-56f1fb043fa0------2","_blank")}},{id:"post-\u52a9\u65590\u5e74\u76ee\u306e\u632f\u308a\u8fd4\u308a",title:'\u52a9\u65590\u5e74\u76ee\u306e\u632f\u308a\u8fd4\u308a <svg width="1.2rem" height="1.2rem" top=".5rem" viewBox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"><path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg>',description:"",section:"Posts",handler:()=>{window.open("https://medium.com/@unilight/%E5%8A%A9%E6%95%990%E5%B9%B4%E7%9B%AE%E3%81%AE%E6%8C%AF%E3%82%8A%E8%BF%94%E3%82%8A-a6bc23dd6253?source=rss-56f1fb043fa0------2","_blank")}},{id:"post-\u9577\u7bc7\u96dc\u8ac7-\u53f0\u7063\u7684\u98df\u7269\u771f\u7684\u597d\u6cb9",title:'[\u9577\u7bc7\u96dc\u8ac7] \u53f0\u7063\u7684\u98df\u7269\u771f\u7684\u597d\u6cb9 <svg width="1.2rem" height="1.2rem" top=".5rem" viewBox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"><path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg>',description:"",section:"Posts",handler:()=>{window.open("https://medium.com/@unilight/%E9%95%B7%E7%AF%87%E9%9B%9C%E8%AB%87-%E5%8F%B0%E7%81%A3%E7%9A%84%E9%A3%9F%E7%89%A9%E7%9C%9F%E7%9A%84%E5%A5%BD%E6%B2%B9-5f6511206048?source=rss-56f1fb043fa0------2","_blank")}},{id:"post-\u540d\u53e4\u5c4b\u5927\u5b78\u7559\u5b78\u56db-amp-\u4e94\u5e74\u76ee-\u540d\u53e4\u5c4b\u5927\u5b78\u52a9\u6559\u96f6\u5e74\u76ee-\u4e0a",title:'\u540d\u53e4\u5c4b\u5927\u5b78\u7559\u5b78\u56db&amp;\u4e94\u5e74\u76ee\u3001\u540d\u53e4\u5c4b\u5927\u5b78\u52a9\u6559\u96f6\u5e74\u76ee\uff08\u4e0a\uff09 <svg width="1.2rem" height="1.2rem" top=".5rem" viewBox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"><path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg>',description:"",section:"Posts",handler:()=>{window.open("https://medium.com/@unilight/%E5%90%8D%E5%8F%A4%E5%B1%8B%E5%A4%A7%E5%AD%B8%E7%95%99%E5%AD%B8%E5%9B%9B-%E4%BA%94%E5%B9%B4%E7%9B%AE-%E5%90%8D%E5%8F%A4%E5%B1%8B%E5%A4%A7%E5%AD%B8%E5%8A%A9%E6%95%99%E9%9B%B6%E5%B9%B4%E7%9B%AE-%E4%B8%8A-c098d3d7eb3d?source=rss-56f1fb043fa0------2","_blank")}},{id:"post-\u540d\u53e4\u5c4b\u5927\u5b78\u7559\u5b78\u4e09\u5e74\u76ee",title:'\u540d\u53e4\u5c4b\u5927\u5b78\u7559\u5b78\u4e09\u5e74\u76ee <svg width="1.2rem" height="1.2rem" top=".5rem" viewBox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"><path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg>',description:"",section:"Posts",handler:()=>{window.open("https://medium.com/%E5%90%8D%E5%8F%A4%E5%B1%8B%E5%A4%A7%E5%AD%B8%E7%95%99%E5%AD%B8%E5%88%86%E4%BA%AB/%E5%90%8D%E5%8F%A4%E5%B1%8B%E5%A4%A7%E5%AD%B8%E7%95%99%E5%AD%B8%E4%B8%89%E5%B9%B4%E7%9B%AE-3b8043847fc0?source=rss-56f1fb043fa0------2","_blank")}},{id:"post-\u540d\u53e4\u5c4b\u5927\u5b78\u7559\u5b78\u4e8c\u5e74\u76ee",title:'\u540d\u53e4\u5c4b\u5927\u5b78\u7559\u5b78\u4e8c\u5e74\u76ee <svg width="1.2rem" height="1.2rem" top=".5rem" viewBox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"><path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg>',description:"",section:"Posts",handler:()=>{window.open("https://medium.com/%E5%90%8D%E5%8F%A4%E5%B1%8B%E5%A4%A7%E5%AD%B8%E7%95%99%E5%AD%B8%E5%88%86%E4%BA%AB/%E5%90%8D%E5%8F%A4%E5%B1%8B%E5%A4%A7%E5%AD%B8%E7%95%99%E5%AD%B8%E4%BA%8C%E9%BB%9E%E4%BA%94%E5%B9%B4%E7%9B%AE-513078f93c3?source=rss-56f1fb043fa0------2","_blank")}},{id:"post-\u540d\u53e4\u5c4b\u5927\u5b78\u7559\u5b78\u4e00\u5e74\u76ee",title:'\u540d\u53e4\u5c4b\u5927\u5b78\u7559\u5b78\u4e00\u5e74\u76ee <svg width="1.2rem" height="1.2rem" top=".5rem" viewBox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"><path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg>',description:"",section:"Posts",handler:()=>{window.open("https://medium.com/%E5%90%8D%E5%8F%A4%E5%B1%8B%E5%A4%A7%E5%AD%B8%E7%95%99%E5%AD%B8%E5%88%86%E4%BA%AB/%E5%90%8D%E5%8F%A4%E5%B1%8B%E5%A4%A7%E5%AD%B8%E7%95%99%E5%AD%B8%E4%B8%80%E5%B9%B4%E7%9B%AE-b72daf4eb9dd?source=rss-56f1fb043fa0------2","_blank")}},{id:"post-ntt-cs-lab\u5be6\u7fd2last-day",title:'NTT CS LAB\u5be6\u7fd2Last Day <svg width="1.2rem" height="1.2rem" top=".5rem" viewBox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"><path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg>',description:"",section:"Posts",handler:()=>{window.open("https://medium.com/ntt-cs-lab%E5%AF%A6%E7%BF%92%E6%97%A5%E8%A8%98-ntt-cs-lab-internship-diary/ntt-cs-lab%E5%AF%A6%E7%BF%92last-day-61c81e7b7816?source=rss-56f1fb043fa0------2","_blank")}},{id:"post-ntt-cs-lab\u5be6\u7fd2day17-19",title:'NTT CS LAB\u5be6\u7fd2Day17\u201319 <svg width="1.2rem" height="1.2rem" top=".5rem" viewBox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"><path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg>',description:"",section:"Posts",handler:()=>{window.open("https://medium.com/ntt-cs-lab%E5%AF%A6%E7%BF%92%E6%97%A5%E8%A8%98-ntt-cs-lab-internship-diary/ntt-cs-lab%E5%AF%A6%E7%BF%92day17-19-e3be2e333762?source=rss-56f1fb043fa0------2","_blank")}},{id:"post-ntt-cs-lab\u5be6\u7fd2day16",title:'NTT CS LAB\u5be6\u7fd2Day16 <svg width="1.2rem" height="1.2rem" top=".5rem" viewBox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"><path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg>',description:"",section:"Posts",handler:()=>{window.open("https://medium.com/ntt-cs-lab%E5%AF%A6%E7%BF%92%E6%97%A5%E8%A8%98-ntt-cs-lab-internship-diary/ntt-cs-lab%E5%AF%A6%E7%BF%92day16-e8394075b5d0?source=rss-56f1fb043fa0------2","_blank")}},{id:"news-one-journal-paper-cdvae-cls-gan-was-accepted-to-the-ieee-transactions-on-emerging-topics-in-computational-intelligence",title:"One journal paper [CDVAE-CLS-GAN] was accepted to the IEEE Transactions on Emerging Topics...",description:"",section:"News"},{id:"news-i-am-co-organizing-the-voice-conversion-challenge-2020-i-developed-a-seq-to-seq-baseline-w-espnet",title:"I am co-organizing the Voice Conversion Challenge 2020. I developed a seq-to-seq baseline...",description:"",section:"News"},{id:"news-one-journal-paper-asvspoof-2019-database-was-accepted-to-the-computer-speech-amp-amp-language",title:"One journal paper [ASVspoof 2019 database] was accepted to the Computer Speech &amp;amp;...",description:"",section:"News"},{id:"news-one-paper-vtn-was-accepted-to-interspeech-2020",title:"One paper [VTN] was accepted to Interspeech 2020.",description:"",section:"News"},{id:"news-the-implementation-of-vtn-is-open-sourced-on-espnet",title:"The implementation of VTN is open-sourced on ESPnet.",description:"",section:"News"},{id:"news-the-proceeding-of-the-joint-workshop-for-the-blizzard-challenge-and-voice-conversion-challenge-2020-is-online-now",title:"The proceeding of the Joint Workshop for the Blizzard Challenge and Voice Conversion...",description:"",section:"News"},{id:"news-four-papers-are-accepted-to-the-joint-workshop-for-the-blizzard-challenge-and-voice-conversion-challenge-2020-challenge-summary-objective-assesement-baseline-asr-tts-nu-entry",title:"Four papers are accepted to the Joint Workshop for the Blizzard Challenge and...",description:"",section:"News"},{id:"news-one-journal-was-accepted-to-the-ieee-acm-transactions-on-audio-speech-and-language-processing-the-early-access-version-is-available-now-on-ieee-xplore-there-is-also-an-arxiv-version",title:"One journal was accepted to the IEEE/ACM Transactions on Audio, Speech, and Language...",description:"",section:"News"},{id:"news-two-first-author-papers-vqvae-vc-bert-asr-were-accepted-to-icassp-2021-also-two-papers-i-co-authored-crank-nonar-seq2seq-vc-were-also-accepted",title:"Two first-author papers [VQVAE-VC] [BERT-ASR] were accepted to ICASSP 2021. Also, two papers...",description:"",section:"News"},{id:"news-one-paper-ema2s-was-accepted-to-ieee-international-symposium-on-circuits-and-systems-iscas-2021",title:"One paper [EMA2S] was accepted to IEEE International Symposium on Circuits and Systems...",description:"",section:"News"},{id:"news-i-successfully-defensed-my-master-s-thesis-also-i-successfully-passed-the-ph-d-entrance-exam-and-will-become-a-ph-d-candidate-at-the-graduate-school-of-informatics-nagoya-university",title:"I successfully defensed my master\u2019s thesis. Also, I successfully passed the Ph.D. entrance...",description:"",section:"News"},{id:"news-one-first-author-paper-dysarthric-vc-w-vtn-vae-was-accepted-to-interspeech-2021-also-one-paper-i-co-authored-relational-data-selection-was-accepted",title:"One first-author paper [Dysarthric VC w/ VTN+VAE] was accepted to Interspeech 2021. Also,...",description:"",section:"News"},{id:"news-you-can-read-some-posts-i-wrote-in-the-blog-page-as-long-as-you-understand-mandarin-chinese",title:"You can read some posts I wrote in the blog page, as long...",description:"",section:"News"},{id:"news-i-started-my-internship-at-facebook-reality-labs-research",title:"I started my internship at Facebook Reality Labs Research.",description:"",section:"News"},{id:"news-three-co-author-papers-were-accepted-to-apsipa-asc-2021-elvc-w-lip-noisy-to-noisy-vc-investigation-of-non-parallel-seq2seq-vc-w-synthetic-data",title:"Three co-author papers were accepted to APSIPA ASC 2021. [ELVC w/ lip] [Noisy-to-noisy...",description:"",section:"News"},{id:"news-one-first-author-paper-prosody-for-asr-tts-vc-was-accepted-to-asru-2021-also-one-paper-i-co-authored-elvc-w-seq2seq-was-accepted",title:"One first-author paper [Prosody for ASR+TTS VC] was accepted to ASRU 2021. Also,...",description:"",section:"News"},{id:"news-received-the-best-paper-award-at-apsipa-asc-2021",title:"Received the Best Paper Award at APSIPA ASC 2021!",description:"",section:"News"},{id:"news-the-first-voicemos-challenge-kicks-off-today-this-is-a-new-challenge-that-aims-to-compare-techniques-for-predicting-the-mean-opinion-score-mos-of-synthetic-speech-we-are-still-accepting-new-challengers-if-you-are-interested-in-participating-please-contact-us-at-voicemos2022-nii-ac-jp",title:"The first VoiceMOS Challenge kicks off today! This is a new challenge that...",description:"",section:"News"},{id:"news-the-voicemos-challenge-was-accepted-as-a-special-session-at-interspeech-2022-again-we-are-still-accepting-new-challengers-if-you-are-interested-in-participating-please-contact-us-at-voicemos2022-nii-ac-jp-first-then-register-at-the-codalab-page",title:"The VoiceMOS Challenge was accepted as a special session at INTERSPEECH 2022! Again,...",description:"",section:"News"},{id:"news-two-first-author-papers-s3prl-vc-ldnet-and-one-co-first-author-paper-n2d-vc-were-accepted-to-icassp-2022-also-two-papers-i-co-authored-mos-finetune-ssl-direct-n2n-vc-were-also-accepted",title:"Two first-author papers [S3PRL-VC] [LDNet] and one co-first author paper [N2D VC] were...",description:"",section:"News"},{id:"news-the-voicemos-challenge-2022-is-over-we-have-a-summary-paper-submitted-to-arxiv-the-codalab-competition-page-is-still-opened-and-anyone-can-register-to-get-the-dataset-and-give-it-a-try",title:"The VoiceMOS Challenge 2022 is over! We have a [summary paper] submitted to...",description:"",section:"News"},{id:"news-i-was-invited-to-give-a-talk-at-\u97f3\u58f0\u8a00\u8a9e\u60c5\u5831\u51e6\u7406\u7814\u7a76\u4f1a-\u97f3\u58f0\u7814\u7a76\u4f1a-slp-sp-a-japanese-domestic-conference-slides-are-here",title:"I was invited to give a talk at \u97f3\u58f0\u8a00\u8a9e\u60c5\u5831\u51e6\u7406\u7814\u7a76\u4f1a/\u97f3\u58f0\u7814\u7a76\u4f1a (SLP/SP), a Japanese domestic...",description:"",section:"News"},{id:"news-i-started-my-internship-at-fair-fundamental-ai-research-meta",title:"I started my internship at FAIR (Fundamental AI Research), Meta.",description:"",section:"News"},{id:"news-two-papers-end-to-end-binaural-synthesis-voicemos-challenge-2022-were-accepted-to-interspeech-2022-also-one-paper-i-co-authored-ssl-for-pathological-asr-was-also-accepted",title:"Two papers [End-to-end binaural synthesis] [VoiceMOS Challenge 2022] were accepted to Interspeech 2022....",description:"",section:"News"},{id:"news-one-paper-expressive-speech-to-speech-translation-was-accepted-to-icassp-2023-also-one-paper-i-co-authored-intermediate-fine-tuning-for-pathological-asr-was-also-accepted",title:"One paper [Expressive Speech-to-Speech Translation] was accepted to ICASSP 2023. Also, one paper...",description:"",section:"News"},{id:"news-one-journal-was-accepted-to-the-ieee-journal-of-selected-topics-in-signal-processing-arxiv-version",title:"One journal was accepted to the IEEE Journal of Selected Topics in Signal...",description:"",section:"News"},{id:"news-the-vtn-journal-paper-received-the-16th-ieee-signal-processing-society-japan-student-best-paper-award-open-access",title:"The VTN journal paper received the 16th IEEE Signal Processing Society Japan Student...",description:"",section:"News"},{id:"news-the-first-singing-voice-conversion-challenge-kicks-off-today-this-is-a-new-version-of-the-voice-conversion-challenge-vcc-series-that-aims-to-compare-techniques-for-singing-voice-conversion-in-contrast-to-normal-voice-conversion-we-are-still-accepting-new-challengers-if-you-are-interested-in-participating-please-fill-in-the-registration-form",title:"The first Singing Voice Conversion Challenge kicks off today! This is a new...",description:"",section:"News"},{id:"news-i-open-sourced-the-s3prl-vc-toolkit-it-also-comes-with-a-huggingface-spaces-demo-please-check-them-out",title:"I open-sourced the s3prl-vc toolkit! It also comes with a HuggingFace Spaces demo....",description:"",section:"News"},{id:"news-i-open-sourced-the-seq2seq-vc-toolkit-it-is-a-toolkit-for-sequence-to-sequence-voice-conversion-research-please-check-it-out",title:"I open-sourced the seq2seq-vc toolkit! It is a toolkit for sequence-to-sequence voice conversion...",description:"",section:"News"},{id:"news-i-start-serving-as-a-student-researcher-at-google-japan",title:"I start serving as a student researcher at Google Japan.",description:"",section:"News"},{id:"news-i-was-honored-the-outstanding-graduate-student-award-\u5b66\u8853\u5968\u52b1\u8cde-of-nagoya-university",title:"I was honored the Outstanding Graduate Student Award (\u5b66\u8853\u5968\u52b1\u8cde) of Nagoya University!",description:"",section:"News"},{id:"news-the-singing-voice-conversion-challenge-2023-is-over-we-have-a-summary-paper-submitted-to-arxiv-there-will-also-be-a-special-session-at-asru-2023",title:"The Singing Voice Conversion Challenge 2023 is over! We have a summary paper...",description:"",section:"News"},{id:"news-a-paper-was-accepted-to-apsipa-asc-2023-evaluate-fac",title:"A paper was accepted to APSIPA ASC 2023. [Evaluate-FAC]",description:"",section:"News"},{id:"news-four-papers-were-presented-at-asru-2023-svcc2023-voicemos-challenge-2023-nu-svcc2023-n2d-vc-gst",title:"Four papers were presented at ASRU 2023. [SVCC2023] [VoiceMOS Challenge 2023] [NU-SVCC2023] [N2D-VC-GST]...",description:"",section:"News"},{id:"news-i-successfully-defended-my-ph-d-thesis",title:"I successfully defended my Ph.D. thesis!",description:"",section:"News"},{id:"news-i-am-now-an-assistant-professor-at-the-graduate-school-of-informatics-nagoya-university",title:"I am now an assistant professor at the Graduate School of Informatics, Nagoya...",description:"",section:"News"},{id:"news-one-paper-was-presented-at-icassp-2024-electrolaryngeal-speech-intelligibility-enhancement-through-robust-linguistic-encoders",title:"One paper was presented at ICASSP 2024. [Electrolaryngeal Speech Intelligibility Enhancement through Robust...",description:"",section:"News"},{id:"news-one-paper-was-accepted-to-ieee-acm-taslp-a-large-scale-evaluation-of-speech-foundation-models",title:"One paper was accepted to IEEE/ACM TASLP. [A Large-Scale Evaluation of Speech Foundation...",description:"",section:"News"},{id:"news-one-paper-was-accepted-to-ieee-acm-taslp-pretraining-and-adaptation-techniques-for-electrolaryngeal-speech-recognition",title:"One paper was accepted to IEEE/ACM TASLP. [Pretraining and Adaptation Techniques for Electrolaryngeal...",description:"",section:"News"},{id:"news-the-voicemos-challenge-2024-is-officially-over-now-you-can-freely-get-the-datasets-by-registering-through-the-codabench-page-there-will-also-be-a-special-session-at-slt-2024",title:"The VoiceMOS Challenge 2024 is officially over! Now you can freely get the...",description:"",section:"News"},{id:"news-we-wrote-a-review-paper-on-evaluation-of-synthesis-speech-which-was-published-at-acoustical-science-and-technology-a-journal-in-japan-the-english-version-can-be-found-here",title:"We wrote a review paper on evaluation of synthesis speech, which was published...",description:"",section:"News"},{id:"news-i-gave-a-lecture-on-voice-conversion-please-find-the-slides-here",title:"I gave a lecture on voice conversion. Please find the slides here.",description:"",section:"News"},{id:"news-i-gave-an-invited-talk-on-voice-conversion-at-citi-academia-sinica-taiwan-please-find-the-slides-here",title:"I gave an invited talk on voice conversion at CITI, Academia Sinica, Taiwan....",description:"",section:"News"},{id:"news-a-paper-was-published-at-ieee-signal-processing-letters-acceptance-rate-10-20-sa-tts",title:"A paper was published at IEEE Signal Processing Letters (acceptance rate: 10-20%). [SA-TTS]...",description:"",section:"News"},{id:"news-i-gave-an-invited-talk-on-voice-conversion-at-sp-ipsj-slp-please-find-the-slides-here",title:"I gave an invited talk on voice conversion at SP/IPSJ-SLP. Please find the...",description:"",section:"News"},{id:"news-a-new-preprint-is-now-available-mos-bench-the-corresponding-open-source-toolkit-sheet-is-also-available-on-github",title:"A new preprint is now available [MOS-Bench]. The corresponding open-source toolkit, [SHEET], is...",description:"",section:"News"},{id:"news-one-paper-was-presented-at-slt2024-voicemos-challenge-2024",title:"One paper was presented at SLT2024. [VoiceMOS Challenge 2024]",description:"",section:"News"},{id:"news-one-paper-was-presented-at-icassp-2025-investigating-factors-related-to-the-naturalness-of-synthesized-unison-singing",title:"One paper was presented at ICASSP 2025. [Investigating Factors Related to the Naturalness...",description:"",section:"News"},{id:"news-the-audiomos-challenge-2025-kicks-off-today-participants-will-build-quality-predictors-for-speehc-music-and-general-audio-we-are-still-accepting-new-challengers-if-you-are-interested-in-participating-please-register-here-and-we-will-contact-you-https-forms-gle-am1qdtewwvmenh5d9",title:"The AudioMOS Challenge 2025 kicks off today! Participants will build quality predictors for...",description:"",section:"News"},{id:"news-the-singing-voice-conversion-challenge-2025-kicks-off-today-this-year-we-focus-on-singing-technique-conversion-if-you-are-interested-in-participating-please-register-here-and-we-will-contact-you-https-forms-gle-gzgawjazvgdk6qkca",title:"The Singing Voice Conversion Challenge 2025 kicks off today! This year we focus...",description:"",section:"News"},{id:"news-i-gave-an-online-invited-talk-at-the-conversational-ai-reading-group-mila-the-topic-was-automatic-quality-assessment-for-speech-and-beyond-slides",title:"I gave an online invited talk at the Conversational AI Reading Group, MILA....",description:"",section:"News"},{id:"news-the-audiomos-challenge-2025-is-officially-over-now-you-can-freely-get-the-datasets-on-the-challenge-page-there-will-also-be-a-special-session-at-asru-2025",title:"The AudioMOS Challenge 2025 is officially over! Now you can freely get the...",description:"",section:"News"},{id:"news-i-gave-a-tutorial-at-interspeech-2025-wiith-erica-cooper-nict-japan-and-jiatong-shi-cmu-usa-on-the-topic-automatic-quality-assessment-for-speech-and-beyond-slides",title:"I gave a tutorial at Interspeech 2025 wiith Erica Cooper (NICT, Japan) and...",description:"",section:"News"},{id:"news-two-papers-sheet-https-arxiv-org-abs-2505-15061-language-independent-speaker-anonymization-https-arxiv-org-abs-2507-00458-were-presented-at-interspeech-2025",title:"Two papers [SHEET](https://arxiv.org/abs/2505.15061) [Language-independent speaker anonymization](https://arxiv.org/abs/2507.00458) were presented at Interspeech 2025.",description:"",section:"News"},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%77%65%6E.%63%68%69%6E%68%75%61%6E%67@%67.%73%70.%6D.%69%73.%6E%61%67%6F%79%61-%75.%61%63.%6A%70","_blank")}},{id:"socials-google-scholar",title:"Google Scholar",section:"Socials",handler:()=>{window.open("https://scholar.google.com/citations?user=g71mJO4AAAAJ","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/unilight","_blank")}},{id:"socials-x",title:"X",description:"Twitter",section:"Socials",handler:()=>{window.open("https://twitter.com/unilightwf","_blank")}},{id:"socials-medium",title:"Medium",section:"Socials",handler:()=>{window.open("https://medium.com/@unilight","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>