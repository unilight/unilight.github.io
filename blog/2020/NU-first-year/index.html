<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> 名古屋大學留學一年目 | Wen-Chin Huang 黃文勁 </title> <meta name="author" content="Wen-Chin Huang"> <meta name="description" content="謹記錄我在名古屋大學情報學研究科碩士課程第一年所領悟到的些許心得。"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://unilight.github.io/blog/2020/NU-first-year/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Wen-Chin Huang 黃文勁 </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching </a> </li> <li class="nav-item "> <a class="nav-link" href="/people/">people </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/publications/">publications</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/projects/">projects</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/blog/">blog</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">名古屋大學留學一年目</h1> <p class="post-meta"> Created in April 22, 2020 </p> <p class="post-tags"> <a href="/blog/2020"> <i class="fa-solid fa-calendar fa-sm"></i> 2020 </a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p><em>註：本文同步張貼於我個人的<a href="https://medium.com/%E5%90%8D%E5%8F%A4%E5%B1%8B%E5%A4%A7%E5%AD%B8%E7%95%99%E5%AD%B8%E5%88%86%E4%BA%AB/%E5%90%8D%E5%8F%A4%E5%B1%8B%E5%A4%A7%E5%AD%B8%E7%95%99%E5%AD%B8%E4%B8%80%E5%B9%B4%E7%9B%AE-b72daf4eb9dd" target="blank" rel="external nofollow noopener">medium</a>。</em></p> <p>我於名古屋大學情報學研究科的碩士課程第一年在前陣子告一段落。異地的生活不比家鄉，每天的生活都是不停地戰鬥，只能像是被遮住兩側視線的馬匹不停向前奔跑，毫無喘息之餘。處在人生精華二十代的我們，時常不知為何而忙，生活是那麼的充實卻又那麼的空虛。然而，在這樣的學年結束之時回首，卻發現始於足下的千里之行，竟也恍恍惚惚的走了好一大段。</p> <p>來日後的研究生活和我想像中沒到完全吻合卻也心滿意足，而那些和我的想像有落差的、沒先計畫好的意料之外，也都自行生長成了一片深刻的回憶。此文算是這一年來的一個總回顧，<strong>主要針對學校生活</strong>。目的上除了紀錄，也希望讓一般人得以窺探相較歐美而言較為冷門的日本留學生活，而未來想來日本留學的後輩若能以此文為參考，我也會是十分感謝的。</p> <h2 id="我念的是什麼系">我念的是什麼系？</h2> <p>我目前就讀的是「名古屋大学 大学院情報学研究科」，大学院=研究所，情報學=資工or資訊or信息(中國說法)，因此以台灣的語言來說就是「名大資工所」。日本大多數研究所會細分所謂的「専攻」，像我的專攻是「知能システム」 ，知能=intelligient，システム=system，白話來說就是人工智慧(AI)。名大比較特別的是，他們稱碩士班/博士班為「博士前期課程/後期課程」，但多數學校則用「修士課程/博士課程」來稱呼。這個program的畢業條件是碩論審查通過+修超過30學分。其中有些學分是不得不參加的「研究室活動」，真正可以自己選的是至少要修14學分以上的「特論」。</p> <h3 id="研究室活動">研究室活動</h3> <ul> <li> <p><strong>seminar</strong> 是各個研究生報告自己研究進度的場合，我們實驗室的話一個人平均一學期發表三次，除了做投影片上台報告，還要把目前的研究進度打成論文的形式，在報告時發給每個人一人一份，報告完還得應付來自博班學長、各教授的刁鑽提問。（為什麼會有「各」教授呢，不像台灣從助理教授開始就要自己帶兵，在日本大多得當到正教授才有自己的實驗室，在那之前就得「寄生」在其他實驗室，所以大的研究室會有五到六位教授）個人認為這個訓練對研究生來說是非常紮實的。</p> </li> <li> <p><strong>輪講</strong> 就是所謂的讀書會，我們實驗室的輪講是一整年讀一本書，這年讀的是一本很難的機器學習的課本（Murphy的MLaPP），一個人負責25–30頁，分兩週共六小時講完。頁數看似很少，但其實準備起來非常麻煩：首先，得把負責的內容從英文翻成日文，再準備講解用的投影片；而我們實驗室有一兩個特別嚴格的博班學長，在你講到一個段落時，他們會舉手問說這個式子是怎麼來的，或者作者這段話是什麼意思，問到他們滿意為止，因此要準備到「不被考倒」所花費的時間十分可觀，我們實驗室的人基本上都一個月前開始準備。</p> </li> <li> <p><strong>meeting</strong>應該是不論國家學校系所，每個研究室都會有的日常活動。我們實驗室是每週一次，每個人都一定要報告這禮拜做了什麼，就算都在忙修課、找工作甚至打工都要大致報告。不同於台灣，日本多數大學生都是要寫簡單的畢業論文（卒論）的，大四就得要進實驗室，因此meeting會有四、碩一及碩二的參加。</p> </li> </ul> <p>除非換實驗室或者考到不同大學，不然碩士論文大多是卒論的延伸，因此可以說是用約三年的時間寫一份碩士論文。雖然是這麼說，但日本大部分的碩士生都無心研究，往上唸的也很少（好吧，台灣也是），大部分的日本碩士生都是當作大學在過（好吧，台灣也是）。碩一在修課、打工，碩二在就活（找工作）中度過，真正花在碩論上的時間很少（總覺得隨便抓一個台大資工畢業的大學生來做他們的碩論應該三個月就做完了）。像上述的seminar，這麼好的制度個人認為真是浪費了。</p> <h3 id="修課">修課</h3> <p>名大是quarter制，也就是一年其實有四個學期，分為春I期、春II期、秋I期、秋II期，一門課一週一個半小時一學分。基本上個人覺得quarter制放在敝系是非常沒有意義的。國外大學會希望採用quarter制，多是基於彈性、減少學生負擔、讓教授可以開一些專門的課等理由。但一來這裡的研究所選修課大部分都比台大的通識課還輕，根本就涼到不行；二來這裡的課都是兩三個教授負責一門課，根本沒教什麼專精的內容。整體而言，單單計算我坐在教室的時間，與學到的內容量根本不成比例。修課可說是我最不滿意敝系的部分。</p> <p>以下列出我有修過的課：</p> <ul> <li>知能システムA1/A2/B1/B2 （春I期、春II期、秋I期、秋II期）<br> 就是系上辦的專題演講，每週聽完後交心得報告（當場交）。學分是四學期一起算的，總共4x8=32次演講中，每交6次心得會有一學分，因此只要聽24次就好；有些人前三學期全勤，第四學期就不來了。我對這門課沒什麼意見啦（反正不管什麼國家，每個系都有），只有當很廢的教授來講很廢的內容，或者心得報告的題目是「請你發想自駕車技術還可以怎麼改善」這種明顯要學生幫想研究題目時，會很不爽而已。</li> <li>音声行動情報処理1/2（春I期、春II期）<br> 這是敝實驗室的教授群開的課，也是唯一一門課標榜著英文授課。主題包含語音處理、自駕車以及運動科學，aka敝實驗室的研究主題。春I期是敝實驗室的各教授每週講一次，春II期是各教授各自請一位外麵的人來講。基本上也是專題演講，就是聽各講者介紹他們自己的研究內容，交心得報告而已。講得極淺，大概是一般理工科來聽都聽得懂的程度，對本科系的我來說，自然是沒學到什麼。</li> <li>知的インタフェース1/2（春I期、春II期）<br> 知的=intelligent，インタフェース=interface，看課名我原本猜教HCI。春I期是敝實驗室的系主任教HCI，好像是一個國內非常大咖的教授，某天上課還秀了他前一天去領獎的照片。課程形式是聽他講課，然後每週有報告要交，題目等級是「找出你生活中的一個好UI/壞UI，分別說明為什麼好/如何改進」這種完全不用CS知識的程度。春II期的教授，從腦波、腦科學，講到perceptron演算法，再講到backpropagation的生物意義，跟長遠差不多鬧。這是唯一一門有期末筆試的課，實驗室的學長姐跟我們說不用準備；硬著頭皮裸考的那天看到題目是「請從生物學解釋人工智慧」，我露出了尷尬又不失禮貌的微笑，亂寫20分鐘走人。總結，什麼都沒學到的課。</li> <li>画像映像情報処理1/2（秋I期、秋II期）<br> 一周兩人，每人報一篇CVPR（一個頂尖會議） paper，報45分鐘。等等，報45分鐘是甚麼概念呢？一篇paper講再細也就15分吧？答案是，剩下30分鐘，全部都拿來讓下面的人問問題；教授會等，等大家問問題問滿45分鐘。說真的，哪來那麼多問題好問呢？因為問問題算分，所以大家不得不問，導致前面大家還會針對報告者沒有講清楚的部分提問，後面真的就是亂問一通。好笑的是，日本人是個很有禮貌的民族，問問題前都會講一段廢話，因此聽起來很白癡：「這可能是個很蠢的問題，但…這個dataset是誰搜集的呢？」或者「抱歉我剛剛可能沒有仔細看，第7頁投影片的圖是截自哪個網站呢？」。雖說如此，但起碼每週都可以讀兩篇CVPR，而且因為有充足的問問題時間，所以日文專業領域口說還很破的我，也藉機練習了不少，真的算是最有收穫的一門課。</li> <li>自然言語処理1/2（秋I期、秋II期）<br> 我人生第一次被當的課。秋I期每週講課，全部都在講機器翻譯，好想問教授，你心中的自然語言處理就只有翻譯了嗎？？？只要交一次報告，內容是教授找了一篇內容農場針對google的機器翻譯的報導，請我們找出裡面講錯的地方，寫完後在某天的課堂上互評。我要上傳時，發現上傳期限已經過了。原來教授說禮拜天deadline的意思是禮拜天0:00截止，但我以為是23:59前交就可以。我竭盡我寥寥無幾的日文能力，寫了信去問可不可以補交，他回我，不行。於是，我就因為這麼糞的報告，被當了。秋II期，換另一個教授每週講課。報告有兩個，第一個是團體報告，題目是「如何將自然語言處理技術導入中學教育」。第二個是，某天上課讓我們打狼人殺（用日文！），然後討論怎麼開發狼人殺chatbot。這就是你日本國立大學資工系的選修課程度嗎……</li> <li>マルチメディア情報処理1/2（秋I期、秋II期）<br> 也是報paper，一周4–5人，每人報一篇AAAI 2019（另一個頂尖會議），聽完要寫回饋單。因為一個人大概只有20分左右的時間，所以講的都比較草率，比較聽不懂。我還記得有一次因為教授要出國所以調課成一週兩次這門課，變成禮拜一先聽這門課的4個人報，禮拜二聽画像課的2個人報，禮拜三再聽這門課4個人報，相當痛苦…</li> </ul> <h2 id="忙嗎">忙嗎？</h2> <h3 id="不不忙">不，不忙。</h3> <p>跟大多數在美國名校打拼，每天唸書寫作業做project之餘還要找實習的朋友相比，我應該是閒到一個爆炸…如上所述，修課外不用寫作業做報告，我就算花了比同期的碩一碩二生們10倍的時間做研究，還是有相當多時間可以利用。平日八點起床吃早餐準備出門，九點半上課或者參加研究室活動，中午跟實驗室的人吃飯或者自己吃，一兩點到四五點上課或者做研究，吃晚餐後看要去練舞上課，還是待在實驗室繼續做研究，然後回家 — 算是把生活過得非常單純無聊。</p> <h4 id="但是">但是，</h4> <p>雖然時間上如此寬鬆，平常生活中要處理的大小壓力還是讓人感到心累，最主要還是來自語言。去區役所辦個文件、回信給教授、問實驗室同學問題，大大小小的地方都得用到日文。更別提學習了，用日文吸收知識是一件非常困難的事：不管是在學校上課，還是像我在外面上跳舞的課，就算每一句話都聽得懂，要整理成自己可以掌握的知識，速度還是相當的慢 — 我覺得用中文/英文/日文學習的效率大概是20:10:1的程度。</p> <hr> <p>這篇文主要講的是我在名古屋大學第一年的學校生活，一些課外的生活，或者人際關係的部分沒有提及，有機會的話再來分享！</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra/">Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2022/displaying-external-posts-on-your-al-folio-blog/">Displaying External Posts on Your al-folio Blog</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2021/NU-second-year/">名古屋大學留學二年目</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Wen-Chin Huang. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. Last updated: August 20, 2024. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?601a2d3465e2a52bec38b600518d5f70"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"about",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-blog",title:"blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-publications",title:"publications",description:"publications by categories in reversed chronological order. generated by jekyll-scholar.",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"nav-projects",title:"projects",description:"A growing collection of your cool projects.",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"nav-repositories",title:"repositories",description:"Edit the `_data/repositories.yml` and change the `github_users` and `github_repos` lists to include your own GitHub profile and repositories.",section:"Navigation",handler:()=>{window.location.href="/repositories/"}},{id:"nav-cv",title:"cv",description:"This is a description of the page. You can modify it in &#39;_pages/cv.md&#39;. You can also change or remove the top pdf download button.",section:"Navigation",handler:()=>{window.location.href="/cv/"}},{id:"nav-teaching",title:"teaching",description:"Materials for courses you taught. Replace this text with your description.",section:"Navigation",handler:()=>{window.location.href="/teaching/"}},{id:"nav-people",title:"people",description:"members of the lab or group",section:"Navigation",handler:()=>{window.location.href="/people/"}},{id:"dropdown-publications",title:"publications",description:"",section:"Dropdown",handler:()=>{window.location.href=""}},{id:"dropdown-projects",title:"projects",description:"",section:"Dropdown",handler:()=>{window.location.href=""}},{id:"dropdown-blog",title:"blog",description:"",section:"Dropdown",handler:()=>{window.location.href="/blog/"}},{id:"post-google-gemini-updates-flash-1-5-gemma-2-and-project-astra",title:'Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra <svg width="1.2rem" height="1.2rem" top=".5rem" viewBox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"><path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg>',description:"We\u2019re sharing updates across our Gemini family of models and a glimpse of Project Astra, our vision for the future of AI assistants.",section:"Posts",handler:()=>{window.open("https://blog.google/technology/ai/google-gemini-update-flash-ai-assistant-io-2024/","_blank")}},{id:"post-displaying-external-posts-on-your-al-folio-blog",title:'Displaying External Posts on Your al-folio Blog <svg width="1.2rem" height="1.2rem" top=".5rem" viewBox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"><path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg>',description:"",section:"Posts",handler:()=>{window.open("https://medium.com/@al-folio/displaying-external-posts-on-your-al-folio-blog-b60a1d241a0a?source=rss-17feae71c3c4------2","_blank")}},{id:"post-\u540d\u53e4\u5c4b\u5927\u5b78\u7559\u5b78\u4e8c\u5e74\u76ee",title:"\u540d\u53e4\u5c4b\u5927\u5b78\u7559\u5b78\u4e8c\u5e74\u76ee",description:"\u5728\u65e5\u672c\u5ff5\u8a08\u7b97\u6a5f\u79d1\u5b78\u535a\u58eb\u662f\u600e\u6a23\u7684\u4e00\u500b\u9ad4\u9a57\uff1f",section:"Posts",handler:()=>{window.location.href="/blog/2021/NU-second-year/"}},{id:"post-\u540d\u53e4\u5c4b\u5927\u5b78\u7559\u5b78\u4e00\u5e74\u76ee",title:"\u540d\u53e4\u5c4b\u5927\u5b78\u7559\u5b78\u4e00\u5e74\u76ee",description:"\u8b39\u8a18\u9304\u6211\u5728\u540d\u53e4\u5c4b\u5927\u5b78\u60c5\u5831\u5b78\u7814\u7a76\u79d1\u78a9\u58eb\u8ab2\u7a0b\u7b2c\u4e00\u5e74\u6240\u9818\u609f\u5230\u7684\u4e9b\u8a31\u5fc3\u5f97\u3002",section:"Posts",handler:()=>{window.location.href="/blog/2020/NU-first-year/"}},{id:"news-one-journal-paper-cdvae-cls-gan-https-arxiv-org-pdf-2001-07849-pdf-was-accepted-to-the-ieee-transactions-on-emerging-topics-in-computational-intelligence",title:"One journal paper [[CDVAE-CLS-GAN](https://arxiv.org/pdf/2001.07849.pdf)] was accepted to the **IEEE Transactions on Emerging Topics...",description:"",section:"News"},{id:"news-i-am-co-organizing-the-voice-conversion-challenge-2020-http-www-vc-challenge-org-i-developed-a-seq-to-seq-baseline-w-espnet-https-github-com-espnet-espnet-tree-master-egs-vcc20",title:"I am co-organizing the [Voice Conversion Challenge 2020](http://www.vc-challenge.org/). I developed a [seq-to-seq baseline...",description:"",section:"News"},{id:"news-one-journal-paper-asvspoof-2019-database-https-www-sciencedirect-com-science-article-abs-pii-s0885230820300474-was-accepted-to-the-computer-speech-amp-language",title:"One journal paper [[ASVspoof 2019 database](https://www.sciencedirect.com/science/article/abs/pii/S0885230820300474)] was accepted to the **Computer Speech &amp;...",description:"",section:"News"},{id:"news-one-paper-vtn-https-unilight-github-io-publication-demos-publications-transformer-vc-was-accepted-to-interspeech-2020",title:"One paper [[VTN](https://unilight.github.io/Publication-Demos/publications/transformer-vc)] was accepted to **Interspeech 2020**.",description:"",section:"News"},{id:"news-the-implementation-of-vtn-https-github-com-espnet-espnet-tree-master-egs-arctic-vc1-is-open-sourced-on-espnet",title:"The implementation of [VTN](https://github.com/espnet/espnet/tree/master/egs/arctic/vc1) is open-sourced on **ESPnet**.",description:"",section:"News"},{id:"news-the-proceeding-https-www-isca-speech-org-archive-vcc-bc-2020-of-the-joint-workshop-for-the-blizzard-challenge-and-voice-conversion-challenge-2020-https-www-synsig-org-index-php-joint-workshop-for-the-blizzard-challenge-and-voice-conversion-challenge-2020-is-online-now",title:"The [proceeding](https://www.isca-speech.org/archive/VCC_BC_2020/) of the [Joint Workshop for the Blizzard Challenge and Voice Conversion...",description:"",section:"News"},{id:"news-four-papers-are-accepted-to-the-joint-workshop-for-the-blizzard-challenge-and-voice-conversion-challenge-2020-https-www-synsig-org-index-php-joint-workshop-for-the-blizzard-challenge-and-voice-conversion-challenge-2020-challenge-summary-https-www-isca-speech-org-archive-vcc-bc-2020-pdfs-vcc2020-paper-13-pdf-objective-assesement-https-www-isca-speech-org-archive-vcc-bc-2020-pdfs-vcc2020-paper-34-pdf-baseline-asr-tts-https-www-isca-speech-org-archive-vcc-bc-2020-pdfs-vcc2020-paper-11-pdf-nu-entry-https-www-isca-speech-org-archive-vcc-bc-2020-pdfs-vcc2020-paper-36-pdf",title:"Four papers are accepted to the [Joint Workshop for the Blizzard Challenge and...",description:"",section:"News"},{id:"news-one-journal-was-accepted-to-the-ieee-acm-transactions-on-audio-speech-and-language-processing-the-early-access-version-https-ieeexplore-ieee-org-document-9314100-is-available-now-on-ieee-xplore-there-is-also-an-arxiv-version-https-arxiv-org-abs-2008-03088",title:"One journal was accepted to the **IEEE/ACM Transactions on Audio, Speech, and Language...",description:"",section:"News"},{id:"news-two-first-author-papers-vqvae-vc-https-arxiv-org-abs-2010-12231-bert-asr-https-arxiv-org-abs-2102-00291-were-accepted-to-icassp-2021-also-two-papers-i-co-authored-crank-https-github-com-k2kobayashi-crank-nonar-seq2seq-vc-https-kan-bayashi-github-io-nonarseq2seqvc-were-also-accepted",title:"Two first-author papers [[VQVAE-VC](https://arxiv.org/abs/2010.12231)] [[BERT-ASR](https://arxiv.org/abs/2102.00291)] were accepted to **ICASSP 2021**. Also, two papers...",description:"",section:"News"},{id:"news-one-paper-ema2s-https-arxiv-org-abs-2102-03786-was-accepted-to-ieee-international-symposium-on-circuits-and-systems-iscas-2021",title:"One paper [[EMA2S](https://arxiv.org/abs/2102.03786)] was accepted to **IEEE International Symposium on Circuits and Systems...",description:"",section:"News"},{id:"news-i-successfully-defensed-my-master-39-s-thesis-assets-pdf-251906183-huang-wenchin-\u4fee\u8ad6-pdf-target-quot-blank-quot-also-i-successfully-passed-the-ph-d-entrance-exam-and-will-become-a-ph-d-candidate-at-the-graduate-school-of-informatics-nagoya-university",title:"I successfully defensed my [master&#39;s thesis](./assets/pdf/251906183\u30fbHUANG-WENCHIN\u30fb\u4fee\u8ad6.pdf){:target=&quot;_blank&quot;}. Also, I successfully passed the Ph.D. entrance...",description:"",section:"News"},{id:"news-one-first-author-paper-dysarthric-vc-w-vtn-vae-https-arxiv-org-abs-2106-01415-was-accepted-to-interspeech-2021-also-one-paper-i-co-authored-relational-data-selection-https-arxiv-org-abs-2106-05629-was-accepted",title:"One first-author paper [[Dysarthric VC w/ VTN+VAE](https://arxiv.org/abs/2106.01415)] was accepted to **Interspeech 2021**. Also,...",description:"",section:"News"},{id:"news-you-can-read-some-posts-i-wrote-in-the-blog-page-as-long-as-you-understand-mandarin-chinese",title:"You can read some posts I wrote in the blog page, as long...",description:"",section:"News"},{id:"news-i-started-my-internship-at-facebook-reality-labs-research",title:"I started my internship at **Facebook Reality Labs Research**.",description:"",section:"News"},{id:"news-three-co-author-papers-were-accepted-to-apsipa-asc-2021-elvc-w-lip-https-arxiv-org-abs-2109-03551-noisy-to-noisy-vc-investigation-of-non-parallel-seq2seq-vc-w-synthetic-data",title:"Three co-author papers were accepted to **APSIPA ASC 2021**. [[ELVC w/ lip](https://arxiv.org/abs/2109.03551)] [Noisy-to-noisy...",description:"",section:"News"},{id:"news-one-first-author-paper-prosody-for-asr-tts-vc-https-arxiv-org-abs-2107-09477-was-accepted-to-asru-2021-also-one-paper-i-co-authored-elvc-w-seq2seq-was-accepted",title:"One first-author paper [[Prosody for ASR+TTS VC](https://arxiv.org/abs/2107.09477)] was accepted to **ASRU 2021**. Also,...",description:"",section:"News"},{id:"news-received-the-best-paper-award-at-apsipa-asc-2021",title:"Received the Best Paper Award at **APSIPA ASC 2021**!",description:"",section:"News"},{id:"news-the-first-voicemos-challenge-https-nii-yamagishilab-github-io-ecooper-demo-voicemos2022-index-html-kicks-off-today-this-is-a-new-challenge-that-aims-to-compare-techniques-for-predicting-the-mean-opinion-score-mos-of-synthetic-speech-we-are-still-accepting-new-challengers-if-you-are-interested-in-participating-please-contact-us-at-voicemos2022-nii-ac-jp-voicemos2022-nii-ac-jp",title:"The first [VoiceMOS Challenge](https://nii-yamagishilab.github.io/ecooper-demo/VoiceMOS2022/index.html) kicks off today! This is a new challenge that...",description:"",section:"News"},{id:"news-the-voicemos-challenge-https-voicemos-challenge-2022-github-io-was-accepted-as-a-special-session-at-interspeech-2022-https-interspeech2022-org-program-special-php-again-we-are-still-accepting-new-challengers-if-you-are-interested-in-participating-please-contact-us-at-voicemos2022-nii-ac-jp-voicemos2022-nii-ac-jp-first-then-register-at-the-codalab-page-https-codalab-lisn-upsaclay-fr-competitions-695",title:"The [VoiceMOS Challenge](https://voicemos-challenge-2022.github.io/) was accepted as a [special session at INTERSPEECH 2022](https://interspeech2022.org/program/special.php)! Again,...",description:"",section:"News"},{id:"news-two-first-author-papers-s3prl-vc-https-arxiv-org-abs-2110-06280-ldnet-https-arxiv-org-abs-2110-09103-and-one-co-first-author-paper-n2d-vc-https-arxiv-org-abs-2110-08213-were-accepted-to-icassp-2022-also-two-papers-i-co-authored-mos-finetune-ssl-https-arxiv-org-abs-2110-02635-direct-n2n-vc-https-arxiv-org-abs-2111-07116-were-also-accepted",title:"Two first-author papers [[S3PRL-VC](https://arxiv.org/abs/2110.06280)] [[LDNet](https://arxiv.org/abs/2110.09103)] and one co-first author paper [[N2D VC](https://arxiv.org/abs/2110.08213)] were...",description:"",section:"News"},{id:"news-the-voicemos-challenge-2022-https-voicemos-challenge-2022-github-io-is-over-we-have-a-summary-paper-https-arxiv-org-abs-2203-11389-submitted-to-arxiv-the-codalab-competition-page-https-codalab-lisn-upsaclay-fr-competitions-695-is-still-opened-and-anyone-can-register-to-get-the-dataset-and-give-it-a-try",title:"The [VoiceMOS Challenge 2022](https://voicemos-challenge-2022.github.io/) is over! We have a [[summary paper](https://arxiv.org/abs/2203.11389)] submitted to...",description:"",section:"News"},{id:"news-i-was-invited-to-give-a-talk-at-\u97f3\u58f0\u8a00\u8a9e\u60c5\u5831\u51e6\u7406\u7814\u7a76\u4f1a-\u97f3\u58f0\u7814\u7a76\u4f1a-slp-sp-a-japanese-domestic-conference-slides-are-here-https-www-slideshare-net-nu-i-todalab-the-voicemos-challenge-2022",title:"I was invited to give a talk at \u97f3\u58f0\u8a00\u8a9e\u60c5\u5831\u51e6\u7406\u7814\u7a76\u4f1a/\u97f3\u58f0\u7814\u7a76\u4f1a (SLP/SP), a Japanese domestic...",description:"",section:"News"},{id:"news-i-started-my-internship-at-fair-fundamental-ai-research-meta",title:"I started my internship at **FAIR (Fundamental AI Research), Meta**.",description:"",section:"News"},{id:"news-two-papers-end-to-end-binaural-synthesis-https-arxiv-org-abs-2207-03697-voicemos-challenge-2022-https-arxiv-org-abs-2203-11389-were-accepted-to-interspeech-2022-also-one-paper-i-co-authored-ssl-for-pathological-asr-https-arxiv-org-abs-2203-15431-was-also-accepted",title:"Two papers [[End-to-end binaural synthesis](https://arxiv.org/abs/2207.03697)] [[VoiceMOS Challenge 2022](https://arxiv.org/abs/2203.11389)] were accepted to **Interspeech 2022**....",description:"",section:"News"},{id:"news-one-paper-expressive-speech-to-speech-translation-https-arxiv-org-abs-2301-10606-was-accepted-to-icassp-2023-also-one-paper-i-co-authored-intermediate-fine-tuning-for-pathological-asr-https-arxiv-org-abs-2211-01079-was-also-accepted",title:"One paper [[Expressive Speech-to-Speech Translation](https://arxiv.org/abs/2301.10606)] was accepted to **ICASSP 2023**. Also, one paper...",description:"",section:"News"},{id:"news-one-journal-was-accepted-to-the-ieee-journal-of-selected-topics-in-signal-processing-arxiv-version-https-arxiv-org-abs-2207-04356",title:"One journal was accepted to the **IEEE Journal of Selected Topics in Signal...",description:"",section:"News"},{id:"news-the-vtn-journal-paper-received-the-16th-ieee-signal-processing-society-japan-student-best-paper-award-open-access-https-ieeexplore-ieee-org-document-9314100",title:"The VTN journal paper received the **16th IEEE Signal Processing Society Japan Student...",description:"",section:"News"},{id:"news-the-first-singing-voice-conversion-challenge-http-www-vc-challenge-org-kicks-off-today-this-is-a-new-version-of-the-voice-conversion-challenge-vcc-series-that-aims-to-compare-techniques-for-singing-voice-conversion-in-contrast-to-normal-voice-conversion-we-are-still-accepting-new-challengers-if-you-are-interested-in-participating-please-fill-in-the-registration-form-https-forms-gle-2xc9vb39vjhx72ha6",title:"The first [Singing Voice Conversion Challenge](http://www.vc-challenge.org/) kicks off today! This is a new...",description:"",section:"News"},{id:"news-i-open-sourced-the-s3prl-vc-https-github-com-unilight-s3prl-vc-toolkit-it-also-comes-with-a-huggingface-spaces-demo-https-huggingface-co-spaces-unilight-s3prl-vc-vcc2020-please-check-them-out",title:"I open-sourced the [**s3prl-vc**](https://github.com/unilight/s3prl-vc) toolkit! It also comes with a [HuggingFace Spaces demo](https://huggingface.co/spaces/unilight/s3prl-vc-vcc2020)....",description:"",section:"News"},{id:"news-i-open-sourced-the-seq2seq-vc-https-github-com-unilight-seq2seq-vc-toolkit-it-is-a-toolkit-for-sequence-to-sequence-voice-conversion-research-please-check-it-out",title:"I open-sourced the [**seq2seq-vc**](https://github.com/unilight/seq2seq-vc) toolkit! It is a toolkit for sequence-to-sequence voice conversion...",description:"",section:"News"},{id:"news-i-start-serving-as-a-student-researcher-at-google-japan",title:"I start serving as a student researcher at **Google Japan**.",description:"",section:"News"},{id:"news-i-was-honored-the-outstanding-graduate-student-award-\u5b66\u8853\u5968\u52b1\u8cde-of-nagoya-university",title:"I was honored the Outstanding Graduate Student Award (\u5b66\u8853\u5968\u52b1\u8cde) of Nagoya University!",description:"",section:"News"},{id:"news-the-singing-voice-conversion-challenge-2023-http-vc-challenge-org-is-over-we-have-a-summary-paper-https-arxiv-org-abs-2306-14422-submitted-to-arxiv-there-will-also-be-a-special-session-at-asru-2023-http-www-asru2023-org-motion-asp-siteid-1007526-amp-menuid-49656-amp-postid-697225-amp-lgid-1",title:"The [Singing Voice Conversion Challenge 2023](http://vc-challenge.org/) is over! We have a [summary paper](https://arxiv.org/abs/2306.14422)...",description:"",section:"News"},{id:"news-a-paper-was-accepted-to-apsipa-asc-2023-evaluate-fac-https-arxiv-org-abs-2309-02133",title:"A paper was accepted to **APSIPA ASC 2023**. [[Evaluate-FAC](https://arxiv.org/abs/2309.02133)]",description:"",section:"News"},{id:"news-four-papers-were-presented-at-asru-2023-svcc2023-https-arxiv-org-abs-2306-14422-voicemos-challenge-2023-https-arxiv-org-abs-2310-02640-nu-svcc2023-https-arxiv-org-abs-2310-05203-n2d-vc-gst-https-arxiv-org-abs-2310-02570",title:"Four papers were presented at **ASRU 2023**. [[SVCC2023](https://arxiv.org/abs/2306.14422)] [[VoiceMOS Challenge 2023](https://arxiv.org/abs/2310.02640)] [[NU-SVCC2023](https://arxiv.org/abs/2310.05203)] [[N2D-VC-GST](https://arxiv.org/abs/2310.02570)]...",description:"",section:"News"},{id:"news-i-successfully-defended-my-ph-d-thesis-assets-pdf-master-thesis-pdf",title:"I successfully defended my [Ph.D. thesis](./assets/pdf/master-thesis.pdf)!",description:"",section:"News"},{id:"news-i-am-now-an-assistant-professor-at-the-graduate-school-of-informatics-nagoya-university",title:"I am now an assistant professor at the Graduate School of Informatics, Nagoya...",description:"",section:"News"},{id:"news-one-paper-was-presented-at-icassp-2024-electrolaryngeal-speech-intelligibility-enhancement-through-robust-linguistic-encoders-https-arxiv-org-abs-2309-09627",title:"One paper was presented at **ICASSP 2024**. [[Electrolaryngeal Speech Intelligibility Enhancement through Robust...",description:"",section:"News"},{id:"news-one-paper-was-accepted-to-ieee-acm-taslp-a-large-scale-evaluation-of-speech-foundation-models-https-arxiv-org-abs-2404-09385",title:"One paper was accepted to **IEEE/ACM TASLP**. [[A Large-Scale Evaluation of Speech Foundation...",description:"",section:"News"},{id:"news-one-paper-was-accepted-to-ieee-acm-taslp-pretraining-and-adaptation-techniques-for-electrolaryngeal-speech-recognition-https-ieeexplore-ieee-org-abstract-document-10533680",title:"One paper was accepted to **IEEE/ACM TASLP**. [[Pretraining and Adaptation Techniques for Electrolaryngeal...",description:"",section:"News"},{id:"news-the-voicemos-challenge-2024-https-sites-google-com-view-voicemos-challenge-past-challenges-voicemos-challenge-2024-is-officially-over-now-you-can-freely-get-the-datasets-by-registering-through-the-codabench-page-https-www-codabench-org-competitions-2650-there-will-also-be-a-special-session-at-slt-2024-https-2024-ieeeslt-org-challenges",title:"The [VoiceMOS Challenge 2024](https://sites.google.com/view/voicemos-challenge/past-challenges/voicemos-challenge-2024) is officially over! Now you can freely get the...",description:"",section:"News"},{id:"news-we-wrote-a-review-paper-on-evaluation-of-synthesis-speech-which-was-published-at-acoustical-science-and-technology-a-journal-in-japan-the-english-version-can-be-found-here-https-www-jstage-jst-go-jp-article-ast-45-4-45-e24-12-article-char-ja",title:"We wrote a review paper on evaluation of synthesis speech, which was published...",description:"",section:"News"},{id:"news-i-gave-a-lecture-on-voice-conversion-please-find-the-slides-assets-pdf-20240716-class-pdf-here",title:"I gave a lecture on voice conversion. Please find the [slides](./assets/pdf/20240716-class.pdf) here.",description:"",section:"News"},{id:"news-i-gave-an-invited-talk-https-www-citi-sinica-edu-tw-assets-htmls-iz240024-zh-html-on-voice-conversion-at-citi-academia-sinica-taiwan-please-find-the-slides-assets-others-20240814-citi-talk-pptx-here",title:"I gave an [invited talk](https://www.citi.sinica.edu.tw/assets/htmls/IZ240024_zh.html) on voice conversion at CITI, Academia Sinica, Taiwan....",description:"",section:"News"},{id:"projects-project-1",title:"project 1",description:"with background image",section:"Projects",handler:()=>{window.location.href="/projects/1_project/"}},{id:"projects-project-2",title:"project 2",description:"a project with a background image and giscus comments",section:"Projects",handler:()=>{window.location.href="/projects/2_project/"}},{id:"projects-project-3-with-very-long-name",title:"project 3 with very long name",description:"a project that redirects to another website",section:"Projects",handler:()=>{window.location.href="/projects/3_project/"}},{id:"projects-project-4",title:"project 4",description:"another without an image",section:"Projects",handler:()=>{window.location.href="/projects/4_project/"}},{id:"projects-project-5",title:"project 5",description:"a project with a background image",section:"Projects",handler:()=>{window.location.href="/projects/5_project/"}},{id:"projects-project-6",title:"project 6",description:"a project with no image",section:"Projects",handler:()=>{window.location.href="/projects/6_project/"}},{id:"projects-project-7",title:"project 7",description:"with background image",section:"Projects",handler:()=>{window.location.href="/projects/7_project/"}},{id:"projects-project-8",title:"project 8",description:"an other project with a background image and giscus comments",section:"Projects",handler:()=>{window.location.href="/projects/8_project/"}},{id:"projects-project-9",title:"project 9",description:"another project with an image \ud83c\udf89",section:"Projects",handler:()=>{window.location.href="/projects/9_project/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%77%65%6E.%63%68%69%6E%68%75%61%6E%67@%67.%73%70.%6D.%69%73.%6E%61%67%6F%79%61-%75.%61%63.%6A%70","_blank")}},{id:"socials-google-scholar",title:"Google Scholar",section:"Socials",handler:()=>{window.open("https://scholar.google.com/citations?user=g71mJO4AAAAJ","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/unilight","_blank")}},{id:"socials-x",title:"X",description:"Twitter",section:"Socials",handler:()=>{window.open("https://twitter.com/unilightwf","_blank")}},{id:"socials-medium",title:"Medium",section:"Socials",handler:()=>{window.open("https://medium.com/@unilight","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>